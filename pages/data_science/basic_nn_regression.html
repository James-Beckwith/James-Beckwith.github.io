<!DOCTYPE html>
<html lang="en">

<head>
    <!-- following line is needed so that plotly can display negative numbers in plots correctly -->
    <meta charset="utf-8">
    <!-- Plotly.js -->
    <!-- the local version requires the use of Plotly.newPlot -->
    <script src="../../scripts/plotly-2.18.2.min.js"></script>
    <!-- <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script> -->
    <!-- <link rel="stylesheet" href="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/default.min.css">
    <script src="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script> -->
    <!-- use prism JS instead, looks nicer, especially the tomorrow dark theme -->
    <link rel="stylesheet" type="text/css" href="../../vendors/css/ionicons.min.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/style2.css">
    <link rel="stylesheet" type="text/css" href="../../vendors/css/prism.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/technical_page.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/queries.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <script>
        MathJax = {
            tex: { inlineMath: [['$in', '$in'], ['\\(', '\\)']] }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    <nav>
        <div class="row">
            <!-- ul is an unordered list which should contain li list elements -->
            <ul class="main-nav js--main-nav">
                <li><a href="../../index.html">Home</a></li>
                <!-- TODO maybe thinking about changing Home to a home icon instead?-->
                <li><a herf="../../about.html">About</a></li>
                <li><a herf="#">Education</a></li>
                <li><a herf="#">Resources</a></li>
            </ul>
            <a class="mobile-nav-icon js--nav-icon"><i class="ion-navicon-round"></i></a>
        </div>
    </nav>

    <div class="main-container">

        <h1 class="knowledge_h1">A linear regression example with a fully connected network</h1>

        <p class="knowledge_p">In this section there is a brief example of solving a multi-feature linear regression
            problem by using a fully connected neural network with 1 layer, so there aren't any hidden layers.
            These kind of problems can also be solved using an inverse theory approach, and can be mathematically
            formulated to be the same assuming that we have linear activation functions on our neurons. See here for
            reference of how the problem of linear regression is solved from an inverse theory perspective:
            <a href="../inverse_theory/inverse_force_to_linear.html">Simple mathematical tricks to force a linear
                problem</a>
        </p>

        <p class="knowledge_p">
            Although this is a basic neural network, it will highlight the main components of creating and training a
            typical neural network:
        <ol>
            <li>Data input and test/train split creation</li>
            <li>Network architecture</li>
            <li>A loss function</li>
            <li>A optimization routine</li>
            <li>Training</li>
            <li>Displaying training results</li>
        </ol>
        </p>

        <h2 class="knowledge_h2">First, import the necessary libraries</h2>

        <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from sklearn import datasets
from sklearn.model_selection import train_test_split</code></pre>

        <h2 class="knowledge_h2">1. Data input and test/train split creation</h2>
        <p class="knowledge_p">
            We'll load a toy dataset from sci-kit learn which has several toy datasets to use. We'll
            use the diabetes dataset which has 442 examples of 10 features with a separate target value. More
            information can be found <a
                href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset">here</a>. We'll also
            use the train_test_split function from sci-kit learn to make our train and test datasets
        </p>

        <pre><code class="language-python"># load the dataset
diabetes = datasets.load_diabetes()
features = diabetes['data']
target = diabetes['target']

# create the train/test split
train_feature, test_feature, train_target, test_target = train_test_split(
     features, target, test_size=0.05, random_state=42)</code></pre>

        <p class="knowledge_p">
            Here we have split the data into train and test datasets using the <i class="i_code">train_test_split
                function</i> from sklearn.model_selection. We also use a fixed random state so that the training and
            test splits are consistent when we re-run the script
        </p>

        <h2 class="knowledge_h2">Network Architecture</h2>

        <p class="knowledge_p">
            Training a model with <i class="i_code">tf.keras</i> typically starts by defining the model
            architecture. Here, we use a <i class="i_code">tf.keras.Sequential</i> model, which <a
                href="https://www.tensorflow.org/guide/keras/sequential_model">represents a sequence of steps</a>. We
            will be training a linear regression model to attempt to map between the input data and the target feature
            using a fully connected layer (<i class="i_code">tf.keras.layers.Dense</i>) with a linear pass through
            activation function.
            If we don't specify an activation function in the creation of the fully connected layer
            (<i class="i_code">layers.Dense</i>), then a linear ($ing(x)=x$in) activation is used. The number of inputs
            can either be
            set by the <i class="i_code">input_shape</i> argument, or automatically when the model is run for the first
            time. Note,
            however, that without defining the input shape we can't show a summary of the model (as the input shape will
            still be unknown to the model until we run data through it).
        </p>

        <pre><code class="language-python">linear_model = tf.keras.Sequential([
    layers.Dense(units=1, input_shape=[10]),
])

# call the summary method of the model to show a summary of the model that we have created
linear_model.summary()
</code></pre>

        <pre><p class="knowledge_pyprint">
            Model: "sequential_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_7 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 11
Trainable params: 11
Non-trainable params: 0
_________________________________________________________________
        </p></pre>

        <h2 class="knowledge_h2">Loss function and optimization</h2>

        <p class="knowledge_p">
            Once we have built the model, we configure the training procedure using the Keras <i
                class="i_code">Model.compile</i>
            method. The most important arguments to compile are the <i class="i_code">loss</i> and the <i
                class="i_code">optimizer</i>, since these
            define the loss function to be optimized which serves as a measure of accuracy for predicted outputs from
            the model (<i class="i_code">mean_absolute_error</i>) and how we should optimize the model based on the
            outputs of this
            loss function (using the <i class="i_code">tf.keras.optimizers.Adam</i> optimizer in this instance). We also
            specify a
            hyper-parameter here for the optimizer in terms of the learning rate, which we'll discuss in the optimizers
            <a href="optimizers.html">topic</a>. This, for the time being is a choice that has been made using prior
            knowledge of what
            works. Hyper-parameter optimization is topic in it's own right and will be covered in a different <a
                href="hyperparameter_optimization.html">topic</a> too.
        </p>

        <pre><code class="language-python">linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.2),
    loss='mean_absolute_error')</code></pre>

        <h2 class="knowledge_h2">Training</h2>

        <p class="knowledge_p">
            Once the model has been built and the training routine defined in the <i class="i_code">compile</i> method
            then we can
            execute training using the <i class="i_code">fit</i> method of our Keras model. Another hyper-parameter of
            note here is the
            number of epochs. This is set manually here, but can also be optimized using early stopping criteria. We
            store the output of the training in the <i class="i_code">history</i> variable to be able to store, analyze,
            and visualize
            metrics about the model training.
        </p>

        <pre><code class="language-python">history = linear_model.fit(
    train_feature,
    train_target,
    epochs=200,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 25% of the training data.
    validation_split = 0.25)</code></pre>

        <h2 class="knowledge_h2">visualization of training metrics</h2>

        <p class="knowledge_p">
            Now that the training has finished we can visualize the model's training progress using the metrics stored
            in the <i>history</i> object:
        </p>

        <pre><code class="language-python">def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.grid(True)

plot_loss(history)</code></pre>

        <img src="../../resources/myImgs/ds/basic_nn_regression_accuracy_plot.png"
            alt="A plot of the evolution of the training and validation set accuracy over the training iterations."
            class="knowledge_img">


        <p class="knowledge_p">
            Now that we have a trained a model, let's see what the predictions look like on our test set. The easiest
            way to do this is to plot the predictions from the test features against the test labels. Ideally, they will
            be the same and we can visually inspect this by plotting them alongside a y=x line.
        </p>

        <pre><code class="language-python">prediction = linear_model.predict(test_feature)
def plot_predictions(target, prediction, xlabel, ylabel):
    min_target = min(target)
    max_target = max(target)
    plt.figure()
    plt.plot(target, prediction, 'x', label='Data')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(f'{ylabel} Vs {xlabel}')
    plt.plot([min_target, max_target],[min_target,max_target],'r--', label='x=y')
    plt.legend()

plot_predictions(test_target, prediction, '', '')</code></pre>

        <img src="../../resources/myImgs/ds/basic_nn_regression_predictions_plot.png"
            alt="A plot of the predicted labels against the test dataset labels." class="knowledge_img">

        <p class="knowledge_p">
            Of course, the predictions aren't perfect and likely never will be for most problems as all recorded data
            has some measurement error. Beyond measurement error, problems that we solve are typically not a result of
            simply physical processes. If they are a result of physical processes they are typically ones we don't fully
            understand (if we did then we wouldn't need a machine learning solution!). Given these two sources of
            uncertainty it is likely that you'll never have a model that can always produce perfect predictions, but you
            should still aim for it. The key is to balance the training time and cost against the accuracy gains you'll
            see. Aim for a model that produces predictions that satisfactory for your problem, trying to attain
            perfection can be expensive and time-consuming.
        </p>

        <h2 class="knowledge_h2">Summary</h2>
        <p class="knowledge_p">
            In this topic we've gone through a simple example of building a fully connected neural regression network. Next, have a look at the maths behind the neural network we've built in this topic: 
        </p>

        <a class="next-page" href="intro_to_nn.html">Basics of Neural Networks</a>

        <div class="license">
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img
                    alt="Creative Commons License" style="border-width:0"
                    src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a><br />This
            work is
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons
                Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </div>

</body>

<!-- use this to run the code syntax highlighting in highlight.js -->
<!-- <script>hljs.highlightAll();</script> -->
<!-- use prism JS instead, looks nicer, especially the tomorrow dark theme -->
<script src="../../scripts/prism.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script src="../../scripts/nav.js"></script>

</html>