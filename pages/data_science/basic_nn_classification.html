<!DOCTYPE html>
<html lang="en">

<head>
    <!-- following line is needed so that plotly can display negative numbers in plots correctly -->
    <meta charset="utf-8">
    <!-- Plotly.js -->
    <!-- the local version requires the use of Plotly.newPlot -->
    <script src="../../scripts/plotly-2.18.2.min.js"></script>
    <!-- <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script> -->
    <!-- <link rel="stylesheet" href="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/default.min.css">
    <script src="http://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script> -->
    <!-- use prism JS instead, looks nicer, especially the tomorrow dark theme -->
    <link rel="stylesheet" type="text/css" href="../../vendors/css/ionicons.min.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/style2.css">
    <link rel="stylesheet" type="text/css" href="../../vendors/css/prism.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/technical_page.css">
    <link rel="stylesheet" type="text/css" href="../../resources/css/queries.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <script>
        MathJax = {
            tex: { inlineMath: [['$in', '$in'], ['\\(', '\\)']] }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    <nav>
        <div class="row">
            <!-- ul is an unordered list which should contain li list elements -->
            <ul class="main-nav js--main-nav">
                <li><a href="../../index.html">Home</a></li>
                <!-- TODO maybe thinking about changing Home to a home icon instead?-->
                <li><a herf="../../about.html">About</a></li>
                <li><a herf="#">Education</a></li>
                <li><a herf="#">Resources</a></li>
            </ul>
            <a class="mobile-nav-icon js--nav-icon"><i class="ion-navicon-round"></i></a>
        </div>
    </nav>

    <div class="main-container">

        <h1 class="knowledge_h1">A fully connected classification network example</h1>

        <p class="knowledge_p">In this section there is a brief example of solving a classification problem with a fully
            connected neural network with one hidden layer. This means that the network is formed of an input layer, a
            hidden layer, and an output layer, with the output of one layer flowing into the next with every neuron of
            one layer connected to every neuron of the next layer, hence fully connected. The main difference between a
            classification and regression network is the use of a softmax activation function to compress the output of
            the final layer into a 0 to 1 range which can be used for prediction of classes, multiple classes would
            typically mean an output vector with probabilities for each class in each element.
        </p>

        <p class="knowledge_p">
            Although this is a basic neural network, it will highlight the main components of creating and training a
            typical neural network:
        <ol>
            <li>Data input and test/train split creation</li>
            <li>Pre-processing</li>
            <li>Network architecture</li>
            <li>A loss function</li>
            <li>A optimization routine</li>
            <li>Training</li>
            <li>Displaying training results</li>
        </ol>
        </p>

        <h2 class="knowledge_h2">First, import the necessary libraries</h2>

        <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds 

from sklearn.model_selection import train_test_split</code></pre>

        <h2 class="knowledge_h2">1. Data input and test/train split creation</h2>
        <p class="knowledge_p">
            We'll load a toy dataset from tensorflow-datasets (a package which must be installed). We are going to use
            the Iris dataset which has 50 examples with 4 features for each of 3 classes of irises. More
            information can be found <a href="https://www.tensorflow.org/datasets/catalog/iris">here</a>.

            We'll also use the train_test_split function from sci-kit learn to make our train and test datasets.
        </p>

        <pre><code class="language-python"># load the dataset from tensorflow datasets
ds = tfds.load('iris', split='train', as_supervised=True)

# !NOTE - the .fit method expects batches so if we don't batch then we'll run into dimension mismatch issues when trying to train
# We'll split the dataset into training and test data and then further split the training dataset into a train and validation dataset
train, test = tf.keras.utils.split_dataset(ds, left_size=0.9, seed=0)
train, val = tf.keras.utils.split_dataset(train, left_size=0.9, seed=0)
train = train.batch(45)
val = val.batch(8)
test = test.batch(8)</code></pre>

        <h2 class="knowledge_h2">Pre-processing</h2>

        <p class="knowledge_p">
            Typically the data that we work with isn't quite perfect for use in our models. We usually apply
            transformations to the data in order to make it optimal for our modelling purposes. The transformations
            could take the form of:

        <ol>
            <li>Formatting</li>
            <li>Encoding</li>
            <li>Normalization</li>
            <li>Feature engineering</li>
            <li>Feature selection</li>
        </ol>

        </p>

        <p class="knowledge_p">
            This is a non-exhaustive list and we'll go through pre-processing in more detail in another notebook. For
            now, we'll just apply a normalization layer to our input features. Because we have a batched input this
            makes
            simple normalization slightly more difficult. For example, do we normalize over the entire dataset or
            individual batches? In many real world scenarios we won't have the full dataset loaded into memory to be
            able to calculate the whole training dataset mean and standard deviation used for normalization. Here, we'll
            use <i class="i_code">BatchNormalization</i> on the input to the model. Given our data is so small, it would
            be better to load
            all the data into memory and normalize over the entire dataset, but we'll stick with the <i
                class="i_code">BatchNormalization</i>
            approach for educational purposes.

            We will define the <i class="i_code">BatchNormalization</i> as a layer when we build our model in Keras.
        </p>

        <h2 class="knowledge_h2">Network Architecture</h2>

        <p class="knowledge_p">
            Training a model with <i class="i_code">tf.keras</i> typically starts by defining the model
            architecture. Here, we use a <i class="i_code">tf.keras.Sequential</i> model, which <a
                href="https://www.tensorflow.org/guide/keras/sequential_model">represents a sequence of steps</a>. We
            will be training a simple classification model to attempt to map between the input features and the target
            labels using fully connected layers (<i class="i_code">tf.keras.layers.Dense</i>) with a ReLU activation
            function for the single hidden layer and a softmax activation function for the output layer.

            The number of inputs can either be set by the <i>input_shape</i> argument, or automatically when the model
            is run for the first time. Note, however, that without defining the input shape then we can't show a summary
            of the model using the <i class="i_code">summary</i> method (as the input shape will still be unknown until
            we run it on data).
        </p>

        <pre><code class="language-python">linear_model = tf.keras.Sequential([
    layers.BatchNormalization(),
    layers.Dense(units=5, input_shape=[4,], activation='relu'),
    layers.Dense(units=3, activation='sigmoid')
])
</code></pre>

        <h2 class="knowledge_h2">Loss function and optimization</h2>

        <p class="knowledge_p">
            Once we have built the architecture of the model, we configure the training procedure using the Keras <i
                class="i_code">Model.compile</i> method. The most important arguments to compile are the <i
                class="i_code">loss</i> and the <i class="i_code">optimizer</i>, since these define what will be
            optimized (<i class="i_code">mean_absolute_error</i> in this example) and how (using the <i
                class="i_code">tf.keras.optimizers.Adam</i>). We also specify a hyper-parameter here for the optimizer
            in terms of the learning rate. This, for the time being is a choice that has been made using prior knowledge
            of what works well. Hyper-parameter optimization is a topic in it's own right and will be covered in a
            different notebook.

            Note that we use the <i class="i_code">SparseCategoricalCrossentropy</i> as our labels as provided as
            integers and not one-hot encoded. Further more, as per the documentation: "There should be # classes
            floating point values per feature for y_pred and a single floating point value per feature for y_true."
            Which is why we have a final output layer with # of classes units.
        </p>

        <pre><code class="language-python">linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.25),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy'])</code></pre>

        <h2 class="knowledge_h2">Training</h2>

        <p class="knowledge_p">
            Once the model has been built and the training routine defined in the <i class="i_code">compile</i> method
            then we can
            execute training using the <i class="i_code">fit</i> method of our Keras model. Another hyper-parameter of
            note here is the
            number of epochs. This is set manually here, but can also be optimized using early stopping criteria. We
            store the output of the training in the <i class="i_code">history</i> variable to be able to store, analyze,
            and visualize
            metrics about the model training.
        </p>


        <pre><code class="language-python">history = linear_model.fit(
    train,
    epochs=200,
    verbose=0,
    validation_data = val
    )</code></pre>

        <p class="knowledge_p">We can now evaluate our trained model on the test dataset. The returned values from the
            <i class="i_code">evaluate</i> method on the model are the loss value and metric values for the model in
            test mode <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate">link to
                documentation</a>.
        </p>

        <pre><code class="language-python">evaluation = linear_model.evaluate(test)
for name, metric in zip(linear_model.metrics_names, evaluation):
    print('test ' + name + ': ' + str(metric))
    </code></pre>

        <pre><p class="knowledge_pyprint">test loss: 0.02136881649494171
test accuracy: 1.0</p></pre>

        <h2 class="knowledge_h2">visualization of training metrics</h2>

        <p class="knowledge_p">
            Now that the training has finished we can visualize the model's training progress using the metrics stored
            in the <i>history</i> object:
        </p>

        <pre><code class="language-python">def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Error')
    plt.legend()
    plt.grid(True)
    plt.ylim([0,2])

plot_loss(history)</code></pre>

        <img src="../../resources/myImgs/ds/basic_nn_classification_accuracy_plot.png"
            alt="A plot of the evolution of the training and validation set loss along with the accuracy of the model over the training iterations."
            class="knowledge_img">

        <p class="knowledge_p">
            Now that we have a trained model we make predictions by passing in our new data.
        </p>

        <pre><code class="language-python">linear_model.predict([[6.4,2.8,5.6, 2.2], [5.4, 3.4,1.7, 0.2], [5.9, 3.,  4.2, 1.5]])</code></pre>

        <pre><p class="knowledge_pyprint">array([[0.   , 0.   , 0.465],
    [1.   , 1.   , 0.039],
    [0.   , 0.994, 0.135]], dtype=float32)</p></pre>

        <h2 class="knowledge_h2">Summary</h2>
        <p class="knowledge_p">
            In this topic we've gone through a simple example of building a fully connected neural classification network. Next, have a look at the maths behind the neural network we've built in this topic: 
        </p>

        <a class="next-page" href="intro_to_nn.html">Basics of Neural Networks</a>

        <div class="license">
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img
                    alt="Creative Commons License" style="border-width:0"
                    src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a><br />This
            work is
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons
                Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </div>

</body>

<!-- use this to run the code syntax highlighting in highlight.js -->
<!-- <script>hljs.highlightAll();</script> -->
<!-- use prism JS instead, looks nicer, especially the tomorrow dark theme -->
<script src="../../scripts/prism.js"></script>

</html>